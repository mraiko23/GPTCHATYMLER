# LLM Service - Unified LLM API wrapper
# Default streaming API with blocking fallback
# call(&block) - streaming by default, blocking if no block given
class LlmService < ApplicationService
  class LlmError < StandardError; end
  class TimeoutError < LlmError; end
  class ApiError < LlmError; end

  def initialize(prompt:, system: nil, **options)
    @prompt = prompt
    @system = system
    @options = options
    @model = options[:model] || ENV.fetch('LLM_MODEL')
    @temperature = options[:temperature]&.to_f || 0.7
    @max_tokens = options[:max_tokens] || 4000
    @timeout = options[:timeout] || 30
    @images = normalize_images(options[:images])
  end

  # Default call - streaming if block given, blocking otherwise
  def call(&block)
    if block_given?
      call_stream(&block)
    else
      call_blocking
    end
  end

  # Explicit blocking call (returns full response)
  def call_blocking
    raise LlmError, "Prompt cannot be blank" if @prompt.blank?

    # Use mock response in development if API key is not configured
    if use_mock_mode?
      return generate_mock_response
    end

    response = make_http_request(stream: false)
    content = response.dig("choices", 0, "message", "content")

    raise LlmError, "No content in response" if content.blank?

    content
  rescue => e
    Rails.logger.error("LLM Error: #{e.class} - #{e.message}")
    raise
  end

  # Explicit streaming call (yields chunks as they arrive)
  def call_stream(&block)
    raise LlmError, "Prompt cannot be blank" if @prompt.blank?
    raise LlmError, "Block required for streaming" unless block_given?

    # Use mock response in development if API key is not configured
    if use_mock_mode?
      return mock_stream_response(&block)
    end

    full_content = ""

    make_http_request(stream: true) do |chunk|
      full_content += chunk
      block.call(chunk)
    end

    full_content
  rescue => e
    Rails.logger.error("LLM Stream Error: #{e.class} - #{e.message}")
    raise
  end

  # Class method shortcuts
  class << self
    # Default: streaming if block, blocking otherwise
    def call(prompt:, system: nil, **options, &block)
      new(prompt: prompt, system: system, **options).call(&block)
    end

    # Explicit blocking call
    def call_blocking(prompt:, system: nil, **options)
      new(prompt: prompt, system: system, **options).call_blocking
    end

    # Explicit streaming call
    def call_stream(prompt:, system: nil, **options, &block)
      new(prompt: prompt, system: system, **options).call_stream(&block)
    end
  end

  private

  def make_http_request(stream: false, &block)
    require 'net/http'
    require 'uri'
    require 'json'

    http, request = prepare_http_request(stream)

    if stream
      handle_stream_response(http, request, &block)
    else
      handle_blocking_response(http, request)
    end
  rescue Net::ReadTimeout
    raise TimeoutError, "Request timed out after #{@timeout}s"
  end

  def prepare_http_request(stream)
    base_url = ENV.fetch('LLM_BASE_URL')
    uri = URI.parse("#{base_url}/chat/completions")

    http = Net::HTTP.new(uri.host, uri.port)
    http.use_ssl = (uri.scheme == 'https')
    http.read_timeout = @timeout
    http.open_timeout = 10

    request = Net::HTTP::Post.new(uri.path)
    request["Content-Type"] = "application/json"
    request["Authorization"] = "Bearer #{api_key}"

    body = build_request_body
    if stream
      request["Accept"] = "text/event-stream"
      body[:stream] = true
    end
    request.body = body.to_json

    [http, request]
  end

  def handle_blocking_response(http, request)
    response = http.request(request)

    case response.code.to_i
    when 200
      JSON.parse(response.body)
    when 429
      raise ApiError, "Rate limit exceeded"
    when 500..599
      raise ApiError, "Server error: #{response.code}"
    else
      raise ApiError, "API error: #{response.code} - #{response.body}"
    end
  rescue JSON::ParserError => e
    raise ApiError, "Invalid JSON response: #{e.message}"
  end

  def handle_stream_response(http, request, &block)
    http.request(request) do |response|
      unless response.code.to_i == 200
        raise ApiError, "API error: #{response.code} - #{response.body}"
      end

      buffer = ""
      response.read_body do |chunk|
        buffer += chunk

        while (line_end = buffer.index("\n"))
          line = buffer[0...line_end].strip
          buffer = buffer[(line_end + 1)..-1]

          next if line.empty?
          next unless line.start_with?("data: ")

          data = line[6..-1]
          next if data == "[DONE]"

          begin
            json = JSON.parse(data)
            if content = json.dig("choices", 0, "delta", "content")
              block.call(content)
            end
          rescue JSON::ParserError => e
            Rails.logger.warn("Failed to parse SSE chunk: #{e.message}")
          end
        end
      end
    end
  end

  def build_request_body
    messages = []
    messages << { role: "system", content: @system } if @system.present?

    if @images.present?
      user_content = []
      user_content << { type: "text", text: @prompt.to_s }
      @images.each do |img|
        user_content << { type: "image_url", image_url: { url: img } }
      end
      messages << { role: "user", content: user_content }
    else
      messages << { role: "user", content: @prompt }
    end

    body = {
      model: @model,
      messages: messages,
      temperature: @temperature,
      max_tokens: @max_tokens
    }

    body
  end

  def api_key
    ENV.fetch('LLM_API_KEY') do
      raise LlmError, "LLM_API_KEY not configured"
    end
  end

  def use_mock_mode?
    Rails.env.development? && ENV['LLM_API_KEY'].blank?
  end

  def mock_stream_response(&block)
    Rails.logger.info("[LLM Mock Mode] Generating mock response")
    
    # Generate a helpful mock response based on the prompt
    response = generate_mock_response
    
    # Simulate streaming by yielding the response in chunks
    response.chars.each_slice(5).each do |chunk_chars|
      chunk = chunk_chars.join
      block.call(chunk)
      sleep(0.02) # Simulate network delay
    end
    
    response
  end

  def generate_mock_response
    # Check if user uploaded images
    if @images.present?
      return "ðŸ–¼ï¸ Ð¯ Ð²Ð¸Ð¶Ñƒ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ! Ð’ Ñ€ÐµÐ¶Ð¸Ð¼Ðµ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð±ÐµÐ· API ÐºÐ»ÑŽÑ‡Ð° Ñ Ð½Ðµ Ð¼Ð¾Ð³Ñƒ Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾ Ð¿Ñ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ. " +
             "Ð§Ñ‚Ð¾Ð±Ñ‹ Ð²ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ AI Vision, Ð´Ð¾Ð±Ð°Ð²ÑŒÑ‚Ðµ LLM_API_KEY Ð² config/application.yml.\n\n" +
             "Ð”Ð»Ñ Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ñ„Ð°Ð¹Ð»Ð¾Ð² - Ð²ÑÑ‘ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð¾Ñ‚Ð»Ð¸Ñ‡Ð½Ð¾! âœ…"
    end

    # Generate contextual response based on prompt
    prompt_lower = @prompt.to_s.downcase
    
    if prompt_lower.include?('Ð¿Ñ€Ð¸Ð²ÐµÑ‚') || prompt_lower.include?('hello') || prompt_lower.include?('Ð·Ð´Ñ€Ð°Ð²ÑÑ‚Ð²')
      return "ðŸ‘‹ ÐŸÑ€Ð¸Ð²ÐµÑ‚! Ð¯ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÑŽ Ð² Ð´ÐµÐ¼Ð¾-Ñ€ÐµÐ¶Ð¸Ð¼Ðµ (Ð±ÐµÐ· API ÐºÐ»ÑŽÑ‡Ð°). Ð¯ Ð¼Ð¾Ð³Ñƒ:\n\n" +
             "âœ… ÐŸÑ€Ð¸Ð½Ð¸Ð¼Ð°Ñ‚ÑŒ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ\n" +
             "âœ… ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°Ñ‚ÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÑƒ Ñ„Ð°Ð¹Ð»Ð¾Ð²\n" +
             "âœ… ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°Ñ‚ÑŒ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ñ‹ Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð²\n\n" +
             "Ð§Ñ‚Ð¾Ð±Ñ‹ Ð²ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ Ð½Ð°ÑÑ‚Ð¾ÑÑ‰Ð¸Ð¹ AI, Ð´Ð¾Ð±Ð°Ð²ÑŒÑ‚Ðµ LLM_API_KEY Ð² config/application.yml"
    elsif prompt_lower.include?('Ñ„Ð°Ð¹Ð»') || prompt_lower.include?('file') || prompt_lower.include?('Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½')
      return "ðŸ“Ž Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ñ„Ð°Ð¹Ð»Ð¾Ð² Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð¾Ñ‚Ð»Ð¸Ñ‡Ð½Ð¾! \n\n" +
             "Ð’ Ñ€ÐµÐ¶Ð¸Ð¼Ðµ Ñ Ð½Ð°ÑÑ‚Ð¾ÑÑ‰Ð¸Ð¼ API ÐºÐ»ÑŽÑ‡Ð¾Ð¼ Ñ ÑÐ¼Ð¾Ð³Ñƒ:\n" +
             "â€¢ ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ (AI Vision)\n" +
             "â€¢ Ð§Ð¸Ñ‚Ð°Ñ‚ÑŒ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ðµ Ñ„Ð°Ð¹Ð»Ñ‹\n" +
             "â€¢ ÐžÑ‚Ð²ÐµÑ‡Ð°Ñ‚ÑŒ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ Ð¾ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ð¼Ð¾Ð¼\n\n" +
             "Ð¡ÐµÐ¹Ñ‡Ð°Ñ Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÑŽ Ð² Ð´ÐµÐ¼Ð¾-Ñ€ÐµÐ¶Ð¸Ð¼Ðµ Ð´Ð»Ñ Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¸Ð½Ñ‚ÐµÑ€Ñ„ÐµÐ¹ÑÐ°."
    elsif prompt_lower.match?(/\?$/)
      return "â“ Ð­Ñ‚Ð¾ Ð¸Ð½Ñ‚ÐµÑ€ÐµÑÐ½Ñ‹Ð¹ Ð²Ð¾Ð¿Ñ€Ð¾Ñ! Ð’ Ð¿Ð¾Ð»Ð½Ð¾Ð¼ Ñ€ÐµÐ¶Ð¸Ð¼Ðµ (Ñ API ÐºÐ»ÑŽÑ‡Ð¾Ð¼) Ñ Ð´Ð°Ð¼ Ñ€Ð°Ð·Ð²Ñ‘Ñ€Ð½ÑƒÑ‚Ñ‹Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚. \n\n" +
             "Ð¡ÐµÐ¹Ñ‡Ð°Ñ Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÑŽ Ð² demo-Ñ€ÐµÐ¶Ð¸Ð¼Ðµ Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ñ‡Ð°Ñ‚Ð°.\n\n" +
             "ðŸ’¡ Ð’ÑÐµ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÑŽÑ‚:\n" +
             "â€¢ ÐžÑ‚Ð¿Ñ€Ð°Ð²ÐºÐ° ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ âœ…\n" +
             "â€¢ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ñ„Ð°Ð¹Ð»Ð¾Ð² âœ…\n" +
             "â€¢ Real-time Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ âœ…\n" +
             "â€¢ WebSocket âœ…"
    else
      return "ðŸ¤– Ð¡Ð¿Ð°ÑÐ¸Ð±Ð¾ Ð·Ð° ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ! Ð¯ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÑŽ Ð² Ð´ÐµÐ¼Ð¾-Ñ€ÐµÐ¶Ð¸Ð¼Ðµ (development mode Ð±ÐµÐ· API ÐºÐ»ÑŽÑ‡Ð°).\n\n" +
             "Ð’Ð°ÑˆÐµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¾ Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð¾. Ð’ Ñ€ÐµÐ¶Ð¸Ð¼Ðµ Ñ Ð½Ð°ÑÑ‚Ð¾ÑÑ‰Ð¸Ð¼ LLM API Ñ ÑÐ¼Ð¾Ð³Ñƒ Ð´Ð°Ñ‚ÑŒ Ð±Ð¾Ð»ÐµÐµ ÑÐ¾Ð´ÐµÑ€Ð¶Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚.\n\n" +
             "Ð”Ð»Ñ Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ñ AI:\n" +
             "1. ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚Ðµ API ÐºÐ»ÑŽÑ‡ (OpenAI, OpenRouter, Ð¸Ð»Ð¸ Ð´Ñ€ÑƒÐ³Ð¾Ð¹ ÑÐµÑ€Ð²Ð¸Ñ)\n" +
             "2. Ð”Ð¾Ð±Ð°Ð²ÑŒÑ‚Ðµ Ð² config/application.yml: LLM_API_KEY: 'your-key'\n" +
             "3. ÐŸÐµÑ€ÐµÐ·Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚Ðµ ÑÐµÑ€Ð²ÐµÑ€\n\n" +
             "âœ¨ Ð’ÑÐµ Ð¾ÑÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÑŽÑ‚ Ð½Ð¾Ñ€Ð¼Ð°Ð»ÑŒÐ½Ð¾!"
    end
  end

  def normalize_images(images)
    return [] if images.blank?
    list = images.is_a?(Array) ? images.compact : [images].compact
    list.map(&:to_s).reject(&:blank?)
  end
end
